
{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate data for 10 manuscripts and 100 readers\n",
    "num_manuscripts = 10\n",
    "num_readers = 100\n",
    "\n",
    "# Step 1: True average scores for each manuscript\n",
    "true_avg_scores = np.random.uniform(50, 90, size=num_manuscripts)\n",
    "reader_skill_levels = np.random.normal(loc=0, scale=5, size=num_readers)\n",
    "\n",
    "reader_predictions = []\n",
    "reader_actuals = []\n",
    "reader_ids = []\n",
    "manuscript_ids = []\n",
    "\n",
    "for m_id, true_avg in enumerate(true_avg_scores):\n",
    "    for r_id in range(num_readers):\n",
    "        noise = np.random.normal(0, 10)\n",
    "        prediction = true_avg + reader_skill_levels[r_id] + noise\n",
    "        prediction = np.clip(prediction, 0, 100)\n",
    "        \n",
    "        reader_predictions.append(prediction)\n",
    "        reader_actuals.append(true_avg)\n",
    "        reader_ids.append(r_id)\n",
    "        manuscript_ids.append(m_id)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    "reader_id": reader_ids,\n",
    "    "manuscript_id": manuscript_ids,\n",
    "    "prediction": reader_predictions,\n",
    "    "true_avg": reader_actuals\n",
    "})\n",
    "\n",
    "# Step 2: Compute reader accuracy and credibility\n",
    "df["abs_error"] = np.abs(df["prediction"] - df["true_avg"])\n",
    "reader_accuracy = df.groupby("reader_id")["abs_error"].mean().reset_index().rename(columns={"abs_error": "mean_absolute_error"})\n",
    "reader_accuracy["credibility_score"] = 100 - reader_accuracy["mean_absolute_error"]\n",
    "\n",
    "# Step 3: Simulate rewards\n",
    "df["reward"] = np.maximum(0, 10 - df["abs_error"])\n",
    "reader_rewards = df.groupby("reader_id")["reward"].sum().reset_index().rename(columns={"reward": "total_reward"})\n",
    "reader_summary = pd.merge(reader_accuracy, reader_rewards, on="reader_id")\n",
    "\n",
    "# Step 4: Early prediction phase\n",
    "df["vote_order"] = df.groupby("manuscript_id").cumcount()\n",
    "df["early_prediction"] = df["vote_order"] < 10\n",
    "early_averages = df[df["early_prediction"]].groupby("manuscript_id")["prediction"].mean().reset_index()\n",
    "early_averages.rename(columns={"prediction": "early_avg_prediction"}, inplace=True)\n",
    "df = df.merge(early_averages, on="manuscript_id", how="left")\n",
    "df["early_deviation"] = np.abs(df["true_avg"] - df["early_avg_prediction"])\n",
    "\n",
    "# Step 5: Add biased readers\n",
    "num_biased_readers = 10\n",
    "biased_readers = np.random.choice(df["reader_id"].unique(), size=num_biased_readers, replace=False)\n",
    "df["biased"] = df["reader_id"].isin(biased_readers)\n",
    "df.loc[df["biased"], "prediction"] = np.clip(df.loc[df["biased"], "true_avg"] + 30, 0, 100)\n",
    "df["abs_error_biased"] = np.abs(df["prediction"] - df["true_avg"])\n",
    "df["reward_biased"] = np.maximum(0, 10 - df["abs_error_biased"])\n",
    "\n",
    "biased_summary = df.groupby("reader_id")[["abs_error_biased", "reward_biased"]].mean().reset_index()\n",
    "biased_summary = biased_summary.rename(columns={\n",
    "    "abs_error_biased": "mean_abs_error_biased",\n",
    "    "reward_biased": "avg_reward_biased"\n",
    "})\n",
    "biased_summary["is_biased"] = biased_summary["reader_id"].isin(biased_readers)\n",
    "\n",
    "# Step 6: Visualization\n",
    "visual_df = pd.merge(reader_summary, biased_summary, on="reader_id")\n",
    "visual_df["is_biased"] = visual_df["is_biased"].astype(bool)\n",
    "\n",
    "# Plot 1: Credibility vs Total Reward\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=visual_df, x="credibility_score", y="total_reward", hue="is_biased", palette={True: "red", False: "blue"})\n",
    "plt.title("Credibility Score vs Total Reward (Biased Readers in Red)")\n",
    "plt.xlabel("Credibility Score")\n",
    "plt.ylabel("Total Reward")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Error Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(visual_df[visual_df["is_biased"] == False]["mean_absolute_error"], color="blue", label="Unbiased", kde=True, stat="density", bins=20)\n",
    "sns.histplot(visual_df[visual_df["is_biased"] == True]["mean_abs_error_biased"], color="red", label="Biased", kde=True, stat="density", bins=20)\n",
    "plt.title("Error Distribution: Biased vs Unbiased Readers")\n",
    "plt.xlabel("Mean Absolute Error")\n",
    "plt.ylabel("Density")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
